<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Normalization | Zhengxin&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="如果是对二维有形式下的各种归一化方式有混淆的，可以参看下面对于 CV 任务的全局概览
{ { &lt; figure src=&ldquo;https://source.unsplash.com/Z0lL0okYjy0&quot; attr=&ldquo;Photo by Aditya Telange on Unsplash&rdquo; align=center link=&ldquo;https://unsplash.com/photos/Z0lL0okYjy0&quot; target=&quot;_blank&rdquo; &gt; } }
Batch Norm Batch Norm
Layer Norm 一维形式 对于 NLP 任务的 一维 embedding，每一个 token 就是不同的 feature。
对同一个 token 位置的所有 feature 进行归一化操作。相当于是把原本长短不一的向量都拉长或缩短到了一个 n 维空间中的球上，n 就是 feature 的维度。不同的 embedding 就是指向角度不同的向量。比如下面这个是二维空间中的形式
二维形式 对于 CV 任务的 二维 embedding，因为单看某个位置的像素其实并没有具体的意义，还需要整张图进行理解，所以往往会将整张图看成一个 $H \times W$ 的feature。不同的 channel 才是不同的 feature。
上面的图相当于一维 Layer Norm 的图绕着垂直于 $feature \times length$ 面的轴旋转90°的结果，channel 数就相当于一维中的 sequence_length。所以二维的 Layer Norm 是对一整张 feature map 去计算均值（mean）和方差（std），然后进行归一化。所以如果输入的 tensor 的维度为 [Batch, Channel, Height, Width] (N, C, H, W)，则计算为： $$ y= \frac{x−E[x]}{Var[x]&#43;ϵ}∗γ&#43;β, \qquad all ~ x ~ with ~ same ~ N ~ index $$ Layer Norm 是在每个样本中，对所有的 channel 和 feature 进行归一化。对于 NLP 任务，就是每个 token (channel) * embedding (feature) 上作归一化。 对于 CV 任务，则是对每一个 channel * feature map 作归一化。">
<meta name="author" content="Luo Zhengxin">
<link rel="canonical" href="https://jensenlzx.github.io/posts/normalization/normalization/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jensenlzx.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jensenlzx.github.io/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jensenlzx.github.io/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jensenlzx.github.io/assets/apple-touch-icon.png">
<link rel="mask-icon" href="https://jensenlzx.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://jensenlzx.github.io/posts/normalization/normalization/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="Normalization" />
<meta property="og:description" content="如果是对二维有形式下的各种归一化方式有混淆的，可以参看下面对于 CV 任务的全局概览
{ { &lt; figure src=&ldquo;https://source.unsplash.com/Z0lL0okYjy0&quot; attr=&ldquo;Photo by Aditya Telange on Unsplash&rdquo; align=center link=&ldquo;https://unsplash.com/photos/Z0lL0okYjy0&quot; target=&quot;_blank&rdquo; &gt; } }
Batch Norm Batch Norm
Layer Norm 一维形式 对于 NLP 任务的 一维 embedding，每一个 token 就是不同的 feature。
对同一个 token 位置的所有 feature 进行归一化操作。相当于是把原本长短不一的向量都拉长或缩短到了一个 n 维空间中的球上，n 就是 feature 的维度。不同的 embedding 就是指向角度不同的向量。比如下面这个是二维空间中的形式
二维形式 对于 CV 任务的 二维 embedding，因为单看某个位置的像素其实并没有具体的意义，还需要整张图进行理解，所以往往会将整张图看成一个 $H \times W$ 的feature。不同的 channel 才是不同的 feature。
上面的图相当于一维 Layer Norm 的图绕着垂直于 $feature \times length$ 面的轴旋转90°的结果，channel 数就相当于一维中的 sequence_length。所以二维的 Layer Norm 是对一整张 feature map 去计算均值（mean）和方差（std），然后进行归一化。所以如果输入的 tensor 的维度为 [Batch, Channel, Height, Width] (N, C, H, W)，则计算为： $$ y= \frac{x−E[x]}{Var[x]&#43;ϵ}∗γ&#43;β, \qquad all ~ x ~ with ~ same ~ N ~ index $$ Layer Norm 是在每个样本中，对所有的 channel 和 feature 进行归一化。对于 NLP 任务，就是每个 token (channel) * embedding (feature) 上作归一化。 对于 CV 任务，则是对每一个 channel * feature map 作归一化。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jensenlzx.github.io/posts/normalization/normalization/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-04-30T17:47:33+08:00" />
<meta property="article:modified_time" content="2024-04-30T17:47:33+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Normalization"/>
<meta name="twitter:description" content="如果是对二维有形式下的各种归一化方式有混淆的，可以参看下面对于 CV 任务的全局概览
{ { &lt; figure src=&ldquo;https://source.unsplash.com/Z0lL0okYjy0&quot; attr=&ldquo;Photo by Aditya Telange on Unsplash&rdquo; align=center link=&ldquo;https://unsplash.com/photos/Z0lL0okYjy0&quot; target=&quot;_blank&rdquo; &gt; } }
Batch Norm Batch Norm
Layer Norm 一维形式 对于 NLP 任务的 一维 embedding，每一个 token 就是不同的 feature。
对同一个 token 位置的所有 feature 进行归一化操作。相当于是把原本长短不一的向量都拉长或缩短到了一个 n 维空间中的球上，n 就是 feature 的维度。不同的 embedding 就是指向角度不同的向量。比如下面这个是二维空间中的形式
二维形式 对于 CV 任务的 二维 embedding，因为单看某个位置的像素其实并没有具体的意义，还需要整张图进行理解，所以往往会将整张图看成一个 $H \times W$ 的feature。不同的 channel 才是不同的 feature。
上面的图相当于一维 Layer Norm 的图绕着垂直于 $feature \times length$ 面的轴旋转90°的结果，channel 数就相当于一维中的 sequence_length。所以二维的 Layer Norm 是对一整张 feature map 去计算均值（mean）和方差（std），然后进行归一化。所以如果输入的 tensor 的维度为 [Batch, Channel, Height, Width] (N, C, H, W)，则计算为： $$ y= \frac{x−E[x]}{Var[x]&#43;ϵ}∗γ&#43;β, \qquad all ~ x ~ with ~ same ~ N ~ index $$ Layer Norm 是在每个样本中，对所有的 channel 和 feature 进行归一化。对于 NLP 任务，就是每个 token (channel) * embedding (feature) 上作归一化。 对于 CV 任务，则是对每一个 channel * feature map 作归一化。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jensenlzx.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Normalization",
      "item": "https://jensenlzx.github.io/posts/normalization/normalization/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Normalization",
  "name": "Normalization",
  "description": "如果是对二维有形式下的各种归一化方式有混淆的，可以参看下面对于 CV 任务的全局概览\n{ { \u0026lt; figure src=\u0026ldquo;https://source.unsplash.com/Z0lL0okYjy0\u0026quot; attr=\u0026ldquo;Photo by Aditya Telange on Unsplash\u0026rdquo; align=center link=\u0026ldquo;https://unsplash.com/photos/Z0lL0okYjy0\u0026quot; target=\u0026quot;_blank\u0026rdquo; \u0026gt; } }\nBatch Norm Batch Norm\nLayer Norm 一维形式 对于 NLP 任务的 一维 embedding，每一个 token 就是不同的 feature。\n对同一个 token 位置的所有 feature 进行归一化操作。相当于是把原本长短不一的向量都拉长或缩短到了一个 n 维空间中的球上，n 就是 feature 的维度。不同的 embedding 就是指向角度不同的向量。比如下面这个是二维空间中的形式\n二维形式 对于 CV 任务的 二维 embedding，因为单看某个位置的像素其实并没有具体的意义，还需要整张图进行理解，所以往往会将整张图看成一个 $H \\times W$ 的feature。不同的 channel 才是不同的 feature。\n上面的图相当于一维 Layer Norm 的图绕着垂直于 $feature \\times length$ 面的轴旋转90°的结果，channel 数就相当于一维中的 sequence_length。所以二维的 Layer Norm 是对一整张 feature map 去计算均值（mean）和方差（std），然后进行归一化。所以如果输入的 tensor 的维度为 [Batch, Channel, Height, Width] (N, C, H, W)，则计算为： $$ y= \\frac{x−E[x]}{Var[x]+ϵ}∗γ+β, \\qquad all ~ x ~ with ~ same ~ N ~ index $$ Layer Norm 是在每个样本中，对所有的 channel 和 feature 进行归一化。对于 NLP 任务，就是每个 token (channel) * embedding (feature) 上作归一化。 对于 CV 任务，则是对每一个 channel * feature map 作归一化。",
  "keywords": [
    
  ],
  "articleBody": "如果是对二维有形式下的各种归一化方式有混淆的，可以参看下面对于 CV 任务的全局概览\n{ { \u003c figure src=“https://source.unsplash.com/Z0lL0okYjy0\" attr=“Photo by Aditya Telange on Unsplash” align=center link=“https://unsplash.com/photos/Z0lL0okYjy0\" target=\"_blank” \u003e } }\nBatch Norm Batch Norm\nLayer Norm 一维形式 对于 NLP 任务的 一维 embedding，每一个 token 就是不同的 feature。\n对同一个 token 位置的所有 feature 进行归一化操作。相当于是把原本长短不一的向量都拉长或缩短到了一个 n 维空间中的球上，n 就是 feature 的维度。不同的 embedding 就是指向角度不同的向量。比如下面这个是二维空间中的形式\n二维形式 对于 CV 任务的 二维 embedding，因为单看某个位置的像素其实并没有具体的意义，还需要整张图进行理解，所以往往会将整张图看成一个 $H \\times W$ 的feature。不同的 channel 才是不同的 feature。\n上面的图相当于一维 Layer Norm 的图绕着垂直于 $feature \\times length$ 面的轴旋转90°的结果，channel 数就相当于一维中的 sequence_length。所以二维的 Layer Norm 是对一整张 feature map 去计算均值（mean）和方差（std），然后进行归一化。所以如果输入的 tensor 的维度为 [Batch, Channel, Height, Width] (N, C, H, W)，则计算为： $$ y= \\frac{x−E[x]}{Var[x]+ϵ}∗γ+β, \\qquad all ~ x ~ with ~ same ~ N ~ index $$ Layer Norm 是在每个样本中，对所有的 channel 和 feature 进行归一化。对于 NLP 任务，就是每个 token (channel) * embedding (feature) 上作归一化。 对于 CV 任务，则是对每一个 channel * feature map 作归一化。\n作用 Layer Norm 把特征进行归一化，使得原本各feature之间的差异进一步放大，形成各多样的表示，从而利于模型的学习。\n比如说，输入的 feature 是 [0.001, 0.008, 0.003] 。对于下一层来说输入值都在 0 附近，那么每个下一层的输出哪怕每一个 feature 的权重不一样，输出的差别也很小。 但是如果使用 Layer Norm 对 feature 进行归一化。整个向量就变成了 [0.08, 0.067, 0.025] 这样所有分量之间的差异就变大了。\n但是这里存在一个问题，这么做的前提条件是 feature 各分量之间仅只有相对大小关系存在意义，而绝对值的大小并没有被模型所捕获。\n也就是说 feature 实际上是在 space 空间中的一个向量。仅有向量的指向具有明确意义，向量的模长是无意义的。这样的归一化方式在 NLP 的 embedding 空间中是合理的，实验效果也不错，但是在每个 feature 绝对值大小有特殊含义的表征上就不是很合理。比如一张纯黑和纯白的图，过 Layer Norm 后都是一致的，但是却显示不了差别，就丧失了部分信息。\n按理说，强化学习 (Reinforcement Learning, RL) 的输入 feature 使用 Batch Norm 针对每个 feature 单独归一化似乎更合理。因为输入的 state 往往是具有一定的物理含义。所以其绝对值的范围相差比较大，Layer Norm 容易出现小绝对值的量被大绝对值的量所 “淹没” 的情况。 比如 [血量, x坐标, y坐标] ，由于血量的值可能在 [0, 100] 范围内，而坐标 在 [0, 1] 范围内。之就会导致 Layer Norm 会一直得到一个近似于 [1, 0, 0] 的向量。 但是如果是 Batch Norm，则网络是和当前位置的其它值进行比较，则不会出现 feature 之间的相互淹没。\n但是实验却发现，RL 使用 Batch Norm 的效果并不好。一种解释是 Batch Norm 需要一个稳定的分布进行输入，而 RL 由于在策略未学好之前，一直在进行探索。所以其输入的分布是极其不稳定的。因而导致 Batch Norm 学到的统计量也十分不稳定。\nInstance Norm Group Norm RMS Norm 同时，Normalization 也把输出的模长归一化，使得梯度不会在\nReference Group Normalization PowerNorm: Rethinking Batch Normalization in Transformers ",
  "wordCount" : "239",
  "inLanguage": "en",
  "datePublished": "2024-04-30T17:47:33+08:00",
  "dateModified": "2024-04-30T17:47:33+08:00",
  "author":{
    "@type": "Person",
    "name": "Luo Zhengxin"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jensenlzx.github.io/posts/normalization/normalization/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Zhengxin's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jensenlzx.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jensenlzx.github.io/" accesskey="h" title="Zhengxin&#39;s Blog (Alt + H)">Zhengxin&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jensenlzx.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://jensenlzx.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Normalization
    </h1>
    <div class="post-meta"><span title='2024-04-30 17:47:33 +0800 CST'>April 30, 2024</span>&nbsp;·&nbsp;Luo Zhengxin

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#batch-norm" aria-label="Batch Norm">Batch Norm</a></li>
                <li>
                    <a href="#layer-norm" aria-label="Layer Norm">Layer Norm</a><ul>
                        
                <li>
                    <a href="#%e4%b8%80%e7%bb%b4%e5%bd%a2%e5%bc%8f" aria-label="一维形式">一维形式</a></li>
                <li>
                    <a href="#%e4%ba%8c%e7%bb%b4%e5%bd%a2%e5%bc%8f" aria-label="二维形式">二维形式</a></li>
                <li>
                    <a href="#%e4%bd%9c%e7%94%a8" aria-label="作用">作用</a></li></ul>
                </li>
                <li>
                    <a href="#instance-norm" aria-label="Instance Norm">Instance Norm</a></li>
                <li>
                    <a href="#group-norm" aria-label="Group Norm">Group Norm</a></li>
                <li>
                    <a href="#rms-norm" aria-label="RMS Norm">RMS Norm</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>如果是对二维有形式下的各种归一化方式有混淆的，可以参看下面对于 <strong>CV</strong> 任务的全局概览</p>
<p>{
{
&lt; figure src=&ldquo;<a href="https://source.unsplash.com/Z0lL0okYjy0%22">https://source.unsplash.com/Z0lL0okYjy0&quot;</a>
attr=&ldquo;Photo by Aditya Telange on Unsplash&rdquo;
align=center
link=&ldquo;<a href="https://unsplash.com/photos/Z0lL0okYjy0%22">https://unsplash.com/photos/Z0lL0okYjy0&quot;</a>
target=&quot;_blank&rdquo; &gt;
}
}</p>
<!-- raw HTML omitted -->
<h2 id="batch-norm">Batch Norm<a hidden class="anchor" aria-hidden="true" href="#batch-norm">#</a></h2>
<p>Batch Norm</p>
<h2 id="layer-norm">Layer Norm<a hidden class="anchor" aria-hidden="true" href="#layer-norm">#</a></h2>
<h3 id="一维形式">一维形式<a hidden class="anchor" aria-hidden="true" href="#一维形式">#</a></h3>
<p>对于 NLP 任务的 <strong>一维</strong> embedding，每一个 token 就是不同的 feature。</p>
<!-- raw HTML omitted -->
<p>对同一个 token 位置的所有 feature 进行归一化操作。相当于是把原本长短不一的向量都拉长或缩短到了一个 n 维空间中的球上，n 就是 feature 的维度。不同的 embedding 就是指向角度不同的向量。比如下面这个是二维空间中的形式</p>
<!-- raw HTML omitted -->
<h3 id="二维形式">二维形式<a hidden class="anchor" aria-hidden="true" href="#二维形式">#</a></h3>
<p>对于 CV 任务的 <strong>二维</strong> embedding，因为单看某个位置的像素其实并没有具体的意义，还需要整张图进行理解，所以往往会将整张图看成一个 $H \times W$ 的feature。不同的 channel 才是不同的 feature。</p>
<!-- raw HTML omitted -->
<p>上面的图相当于一维 Layer Norm 的图绕着垂直于 $feature \times length$ 面的轴旋转90°的结果，channel 数就相当于一维中的 sequence_length。所以二维的 Layer Norm 是对一整张 feature map 去计算均值（mean）和方差（std），然后进行归一化。所以如果输入的 tensor 的维度为 <code>[Batch, Channel, Height, Width]</code> (N, C, H, W)，则计算为：
$$
y= \frac{x−E[x]}{Var[x]+ϵ}∗γ+β, \qquad all ~ x ~ with ~ same ~ N ~ index
$$
Layer Norm 是在每个样本中，对所有的 channel 和 feature 进行归一化。对于 NLP 任务，就是每个 token (channel) * embedding (feature) 上作归一化。 对于 CV 任务，则是对每一个 channel * feature map 作归一化。</p>
<h3 id="作用">作用<a hidden class="anchor" aria-hidden="true" href="#作用">#</a></h3>
<p>Layer Norm 把特征进行归一化，使得原本各feature之间的差异进一步放大，形成各多样的表示，从而利于模型的学习。</p>
<p>比如说，输入的 feature 是 <code>[0.001, 0.008, 0.003]</code> 。对于下一层来说输入值都在 <code>0</code> 附近，那么每个下一层的输出哪怕每一个 feature 的权重不一样，输出的差别也很小。
但是如果使用 Layer Norm 对 feature 进行归一化。整个向量就变成了 <code>[0.08, 0.067, 0.025]</code> 这样所有分量之间的差异就变大了。</p>
<p>但是这里存在一个问题，这么做的前提条件是 feature 各分量之间仅只有相对大小关系存在意义，而绝对值的大小并没有被模型所捕获。</p>
<!-- raw HTML omitted -->
<p>也就是说 feature 实际上是在 space 空间中的一个向量。仅有向量的指向具有明确意义，向量的模长是无意义的。这样的归一化方式在 NLP 的 embedding 空间中是合理的，实验效果也不错，但是在每个 feature 绝对值大小有特殊含义的表征上就不是很合理。比如一张纯黑和纯白的图，过 Layer Norm 后都是一致的，但是却显示不了差别，就丧失了部分信息。</p>
<p>按理说，强化学习 (Reinforcement Learning, RL) 的输入 feature 使用 Batch Norm 针对每个 feature 单独归一化似乎更合理。因为输入的 state 往往是具有一定的物理含义。所以其绝对值的范围相差比较大，Layer Norm 容易出现小绝对值的量被大绝对值的量所 “淹没” 的情况。
比如 <code>[血量, x坐标, y坐标]</code> ，由于<code>血量</code>的值可能在 <code>[0, 100]</code> 范围内，而<code>坐标</code> 在 <code>[0, 1]</code> 范围内。之就会导致 Layer Norm 会一直得到一个近似于 <code>[1, 0, 0]</code> 的向量。
但是如果是 Batch Norm，则网络是和当前位置的其它值进行比较，则不会出现 feature 之间的相互淹没。</p>
<p>但是实验却发现，RL 使用 Batch Norm 的效果并不好。一种解释是 Batch Norm 需要一个稳定的分布进行输入，而 RL 由于在策略未学好之前，一直在进行探索。所以其输入的分布是极其不稳定的。因而导致 Batch Norm 学到的统计量也十分不稳定。</p>
<h2 id="instance-norm">Instance Norm<a hidden class="anchor" aria-hidden="true" href="#instance-norm">#</a></h2>
<h2 id="group-norm">Group Norm<a hidden class="anchor" aria-hidden="true" href="#group-norm">#</a></h2>
<h2 id="rms-norm">RMS Norm<a hidden class="anchor" aria-hidden="true" href="#rms-norm">#</a></h2>
<p>同时，Normalization 也把输出的模长归一化，使得梯度不会在</p>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<ol>
<li><a href="https://arxiv.org/abs/1803.08494">Group Normalization</a></li>
<li><a href="https://proceedings.mlr.press/v119/shen20e/shen20e.pdf">PowerNorm: Rethinking Batch Normalization in Transformers</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://jensenlzx.github.io/">Zhengxin&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
